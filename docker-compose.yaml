version: "3"

services:

  mycluster-master:
    image: popbones/spark-hadoop-cluster
    build:
      context: .
      dockerfile: hadoop.Dockerfile
    depends_on:
      - mycluster-slave-1
      - mycluster-slave-2
    hostname: mycluster-master
    ports:
      - "8080:8080"
      - "8088:8088"
      - "50070:50070"
      - "50090:50090"
      - "9000:9000"
    working_dir: /usr/local/spark
    entrypoint: bash
    command:
      - -c
      - service ssh start && /usr/local/hadoop/spark-services.sh && tail -f /dev/null
    volumes:
      - "$PWD/testdata/HelloWorld.jar:/testdata/HelloWorld.jar"

  mycluster-slave-1:
    image: popbones/spark-hadoop-cluster
    build:
      context: .
      dockerfile: hadoop.Dockerfile
    hostname: mycluster-slave-1
    ports:
      - "8041:8042"
    entrypoint: bash
    command:
      - -c
      - service ssh start && tail -f /dev/null
    volumes:
      - "$PWD/testdata/HelloWorld.jar:/testdata/HelloWorld.jar"

  mycluster-slave-2:
    image: popbones/spark-hadoop-cluster
    build:
      context: .
      dockerfile: hadoop.Dockerfile
    hostname: mycluster-slave-2
    ports:
      - "8042:8042"
    entrypoint: bash
    command:
      - -c 
      - service ssh start && tail -f /dev/null
    volumes:
      - "$PWD/testdata/HelloWorld.jar:/testdata/HelloWorld.jar"
  
  apache-livy:
    image: popbones/livy
    build:
      context: .
      dockerfile: livy.Dockerfile
    depends_on:
      - mycluster-master
    hostname: apache-livy
    ports:
      - 8998:8998
    environment:
      SPARK_CONF_DIR: /etc/spark
      HADOOP_CONF_DIR: /etc/hadoop
    volumes:
      - "$PWD/config/livy/livy.conf:/usr/local/livy/conf/livy.conf"
      - "$PWD/config/hadoop:/etc/hadoop"
      - "$PWD/config/spark:/etc/spark"
